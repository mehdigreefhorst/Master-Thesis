{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc1d9ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd20c444",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"http://127.0.0.1:5001\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efe8b5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def signup():\n",
    "\n",
    "    signup_endpoint = f\"{endpoint}/auth/signup\"\n",
    "    email = \"test@test.nl\"\n",
    "    password = \"tester123\"\n",
    "    login_data = {\"email\": email, \"password\": password}\n",
    "\n",
    "    response = requests.post(signup_endpoint, json=login_data)\n",
    "    returnable  = response.json()\n",
    "    print(returnable)\n",
    "\n",
    "    login_endpoint = f\"{endpoint}/auth/login\"\n",
    "\n",
    "    response = requests.post(login_endpoint, json=login_data)\n",
    "    access_token = response.json()[\"access_token\"]\n",
    "    print(\"access_token = \", access_token)\n",
    "    return returnable, access_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cfdd578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import Dict, Optional\n",
    "\n",
    "class ApplicationTester:\n",
    "    endpoint = \"http://127.0.0.1:5001\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.access_token: Optional[str] = None\n",
    "        self.refresh_token: Optional[str] = None\n",
    "        self.session = requests.Session()\n",
    "        self.login()\n",
    "\n",
    "    # ---------- auth ----------\n",
    "    def login(self):\n",
    "        login_endpoint = f\"{self.endpoint}/auth/login\"\n",
    "        data = {\"email\": \"test@test.nl\", \"password\": \"tester123\"}\n",
    "        r = self.session.post(login_endpoint, json=data)\n",
    "        r.raise_for_status()\n",
    "        payload = r.json()\n",
    "        # Keep both tokens\n",
    "        self.access_token = payload[\"access_token\"]\n",
    "        self.refresh_token = payload[\"refresh_token\"]\n",
    "\n",
    "    def _auth_header(self, use_refresh: bool = False) -> Dict[str, str]:\n",
    "        token = self.refresh_token if use_refresh else self.access_token\n",
    "        return {\"Authorization\": f\"Bearer {token}\"} if token else {}\n",
    "\n",
    "    def refresh(self) -> bool:\n",
    "        \"\"\"Use the refresh token to get a new pair of tokens. Returns True on success.\"\"\"\n",
    "        refresh_endpoint = f\"{self.endpoint}/auth/refresh\"\n",
    "        r = self.session.post(refresh_endpoint, headers=self._auth_header(use_refresh=True))\n",
    "        if r.status_code == 200:\n",
    "            payload = r.json()\n",
    "            self.access_token = payload[\"access_token\"]\n",
    "            self.refresh_token = payload[\"refresh_token\"]\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    # ---------- request wrapper that auto-refreshes ----------\n",
    "    def _request(self, method: str, path: str, **kwargs):\n",
    "        if not path.startswith(\"/\"):\n",
    "            raise ValueError(\"Path should start with /\")\n",
    "        url = f\"{self.endpoint}{path}\"\n",
    "\n",
    "        # 1st attempt with access token\n",
    "        headers = kwargs.pop(\"headers\", {})\n",
    "        headers.update(self._auth_header())\n",
    "        resp = self.session.request(method, url, headers=headers, **kwargs)\n",
    "\n",
    "        # If access token expired, try once to refresh and retry\n",
    "        if resp.status_code == 401:\n",
    "            # Optional: check msg to ensure itâ€™s token-expired, not other auth error\n",
    "            try:\n",
    "                msg = resp.json().get(\"msg\", \"\").lower()\n",
    "            except Exception:\n",
    "                msg = \"\"\n",
    "            if \"token has expired\" in msg or \"signature verification failed\" in msg or \"not fresh\" in msg or not msg:\n",
    "                if self.refresh():\n",
    "                    headers = kwargs.get(\"headers\", {})\n",
    "                    headers.update(self._auth_header())\n",
    "                    return self.session.request(method, url, headers=headers, **kwargs)\n",
    "        return resp\n",
    "\n",
    "    # ---------- public HTTP helpers ----------\n",
    "    def post(self, path: str, json_data: Dict):\n",
    "        return self._request(\"POST\", path, json=json_data)\n",
    "\n",
    "    def get(self, path: str, search_params=None):\n",
    "        return self._request(\"GET\", path, params=search_params)\n",
    "\n",
    "    def put(self, path: str, json_data: Dict):\n",
    "        return self._request(\"PUT\", path, json=json_data)\n",
    "\n",
    "app = ApplicationTester()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a61340",
   "metadata": {},
   "source": [
    "### View all prior made scraper cluster instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "793e5aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cluster_entity_id': '68e10e7f9f0e01b7def2e413',\n",
       "  'created_at': '2025-10-04T12:09:12.453000+00:00',\n",
       "  'deleted_at': None,\n",
       "  'id': '68e10e689f0e01b7def2e411',\n",
       "  'scraper_entity_id': '68e10e699f0e01b7def2e412',\n",
       "  'stages': {'cluster_enrich': 'initialized',\n",
       "   'cluster_prep': 'completed',\n",
       "   'clustering': 'initialized',\n",
       "   'initialized': 'completed',\n",
       "   'scraping': 'completed'},\n",
       "  'updated_at': '2025-10-04T12:10:05.342000+00:00',\n",
       "  'user_id': '68ac7022e24c87692ba648f4'},\n",
       " {'cluster_entity_id': '68e13064c04b5f3e7ed60c9d',\n",
       "  'created_at': '2025-10-04T14:32:43.391000+00:00',\n",
       "  'deleted_at': None,\n",
       "  'id': '68e1300bc04b5f3e7ed60c93',\n",
       "  'scraper_entity_id': '68e1300fc04b5f3e7ed60c94',\n",
       "  'stages': {'cluster_enrich': 'initialized',\n",
       "   'cluster_prep': 'completed',\n",
       "   'clustering': 'initialized',\n",
       "   'initialized': 'completed',\n",
       "   'scraping': 'completed'},\n",
       "  'updated_at': '2025-10-04T14:34:41.389000+00:00',\n",
       "  'user_id': '68ac7022e24c87692ba648f4'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.get(\"/scraper_cluster\").json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7667d3a5",
   "metadata": {},
   "source": [
    "### Create a new instance of the scraper cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca25b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scraper_cluster_id': '68e1300bc04b5f3e7ed60c93'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraper_cluster_instance = app.post(\"/scraper_cluster\", json_data={}).json()\n",
    "scraper_cluster_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acd5c700",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper_cluster_instance = {'scraper_cluster_id': '68e1300bc04b5f3e7ed60c93'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4814dc8",
   "metadata": {},
   "source": [
    "### Create the scraping stage, where you define the keywords and the subreddits to look for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5d4735",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper_request = {\"keywords\": [\"subtitles\", \"video\", \"grammar\"], \"subreddits\": [\"deaf\", \"asl\", \"bsl\"]}\n",
    "scraper_request[\"scraper_cluster_id\"] = scraper_cluster_instance[\"scraper_cluster_id\"]\n",
    "print(scraper_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33e1c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'keywords': ['subtitles', 'video', 'grammar'], 'subreddits': ['deaf', 'asl', 'bsl'], 'scraper_cluster_id': '68e1300bc04b5f3e7ed60c93'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "scraper_instance = app.post(\"/scraper\", json_data=scraper_request).json()\n",
    "scraper_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "965acc42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scraper_id': '68e1300fc04b5f3e7ed60c94'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraper_instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ab47e2",
   "metadata": {},
   "source": [
    "### Start the scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "405c25ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'successfully scraped the scraper instance on reddit',\n",
       " 'paused': False,\n",
       " 'processed': 9,\n",
       " 'total': 9}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_scraper = app.put(\"/scraper/start\", {\"scraper_cluster_id\": scraper_request[\"scraper_cluster_id\"]}).json()\n",
    "response_scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988df64d",
   "metadata": {},
   "source": [
    "### Create a single dimension of the posts and comments in an unnested system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36ee011",
   "metadata": {},
   "source": [
    "# :TODO Right now we don't do anything with user provided media. SO if the message was actually a picture or video, we disregard it. Which is bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81c366fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'preparing the cluster is successful, a total of 5819 cluster units are created'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_cluster_prep = app.post(\"/clustering/prepare_cluster\", {\"scraper_cluster_id\": scraper_request[\"scraper_cluster_id\"]}).json()\n",
    "response_cluster_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f758b1",
   "metadata": {},
   "source": [
    "### Here should come the step for calling the route to create LLM summaries of the comment threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4bf7af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'preparing the cluster is successful, a total of 2532 cluster units are created'}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO do this later first make the baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b651e3",
   "metadata": {},
   "source": [
    "### Call the clustering route to cluster all the documents that are found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34b89cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_unit_entities = app.post(\"/clustering/get_cluster_units\", json_data={\"scraper_cluster_id\": scraper_request[\"scraper_cluster_id\"]})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4fbd3dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2536"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cluster_unit_entities.json()[\"cluster_unit_entities\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de425f40",
   "metadata": {},
   "source": [
    "### We are now saving the data in a local format. So now we go futher in a different notebook for testing.\n",
    "\n",
    "The size of the json is only 6 MB for 2532 cluster units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ed566dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"testfile.json\", \"w\") as f:\n",
    "    f.write(json.dumps(cluster_unit_entities.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3c5f2b",
   "metadata": {},
   "source": [
    "### Call the Q&A route to get to the insights of the scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24caecf4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
